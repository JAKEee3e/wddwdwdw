{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Manga AI\n",
    "\n",
    "This notebook launches a fully-local pipeline: **Qwen2.5 → SDXL → SAM → SDXL Inpaint → Page Composer** and exposes a public HTTPS URL via **Cloudflare Tunnel**.\n",
    "\n",
    "Model folders:\n",
    "- `models/qwen2.5/`\n",
    "- `models/sdxl/`\n",
    "- `models/sdxl_inpaint/`\n",
    "- `models/sam/` (checkpoint file expected: `sam_vit_h_4b8939.pth`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb8c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Runtime settings (run this BEFORE loading any torch/diffusers models)\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "# On L40S (48GB), it's safe to keep SAM on GPU. Override if needed: \"cpu\"\n",
    "os.environ.setdefault(\"MANGA_AI_SAM_DEVICE\", \"cuda\")\n",
    "\n",
    "# Use Animagine XL 4.0 as the SDXL art model (Diffusers repo)\n",
    "os.environ.setdefault(\"MANGA_AI_SDXL_REPO\", \"cagliostrolab/animagine-xl-4.0\")\n",
    "\n",
    "print(\"PYTORCH_CUDA_ALLOC_CONF=\", os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\"))\n",
    "print(\"MANGA_AI_SAM_DEVICE=\", os.environ.get(\"MANGA_AI_SAM_DEVICE\"))\n",
    "print(\"MANGA_AI_SDXL_REPO=\", os.environ.get(\"MANGA_AI_SDXL_REPO\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect paths\n",
    "CWD = Path.cwd().resolve()\n",
    "ROOT = CWD\n",
    "\n",
    "# If we opened this notebook from inside ./manga_ai, ROOT is already the package folder.\n",
    "# If we opened it from repo root, ROOT needs to be ./manga_ai.\n",
    "if not ((ROOT / \"models\").exists() and (ROOT / \"scripts\").exists()):\n",
    "    if (ROOT / \"manga_ai\" / \"models\").exists() and (ROOT / \"manga_ai\" / \"scripts\").exists():\n",
    "        ROOT = (ROOT / \"manga_ai\").resolve()\n",
    "\n",
    "REPO_ROOT = ROOT.parent\n",
    "\n",
    "# Make sure we can `import manga_ai` regardless of where Jupyter was launched from.\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "print(\"CWD:\", CWD)\n",
    "print(\"ROOT (manga_ai folder):\", ROOT)\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"Python:\", sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"NumPy:\", np.__version__)\n",
    "\n",
    "# If you see NumPy 2.x here, you may hit binary incompatibility crashes with matplotlib/scikit-learn.\n",
    "# Fix by running the following in a cell, then RESTART the kernel:\n",
    "# !pip install \"numpy<2\" --force-reinstall\n",
    "# !pip install -r manga_ai/requirements.txt --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from manga_ai.scripts.model_downloader import ensure_models_downloaded\n",
    "from manga_ai.scripts.pipeline import default_model_paths\n",
    "\n",
    "paths = default_model_paths(ROOT)\n",
    "\n",
    "# If using an alternative SDXL repo, keep its files in a dedicated folder to avoid mixing models.\n",
    "os.environ.setdefault(\"MANGA_AI_SDXL_DIR\", str(ROOT / \"models\" / \"animagine_xl_4.0\"))\n",
    "\n",
    "print(\"Qwen dir:\", paths.qwen_dir)\n",
    "print(\"SDXL dir:\", paths.sdxl_dir)\n",
    "print(\"SDXL inpaint dir:\", paths.sdxl_inpaint_dir)\n",
    "print(\"SAM ckpt:\", paths.sam_ckpt)\n",
    "\n",
    "# This will download models into the folders above if they are missing.\n",
    "# For gated HuggingFace models, set: MANGA_AI_HF_TOKEN\n",
    "try:\n",
    "    ensure_models_downloaded(\n",
    "        qwen_dir=paths.qwen_dir,\n",
    "        sdxl_dir=paths.sdxl_dir,\n",
    "        sdxl_inpaint_dir=paths.sdxl_inpaint_dir,\n",
    "        sam_ckpt=paths.sam_ckpt,\n",
    "        hf_token=os.environ.get(\"MANGA_AI_HF_TOKEN\"),\n",
    "    )\n",
    "    print(\"Models are present (downloaded if needed).\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"Model download/setup failed. If SDXL/Qwen are gated, accept the license on HuggingFace and set MANGA_AI_HF_TOKEN. \"\n",
    "        f\"Original error: {e}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3742eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Quick sanity checks (doesn't load the full ML pipelines)\n",
    "assert (paths.qwen_dir / \"config.json\").exists(), \"Qwen config.json not found\"\n",
    "assert (paths.sdxl_dir / \"model_index.json\").exists(), \"SDXL model_index.json not found\"\n",
    "assert (paths.sdxl_inpaint_dir / \"model_index.json\").exists(), \"SDXL inpaint model_index.json not found\"\n",
    "assert paths.sam_ckpt.exists(), \"SAM checkpoint not found\"\n",
    "\n",
    "print(\"Sanity checks passed.\")\n",
    "print(\"SAM checkpoint size (MB):\", round(paths.sam_ckpt.stat().st_size / (1024 * 1024), 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Web UI + Cloudflare Public URL\n",
    "\n",
    "Running the next cell will:\n",
    "- Start Gradio locally on port `7860`\n",
    "- Start a Cloudflare tunnel and print a public `https://...trycloudflare.com` URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from manga_ai.scripts.cloudflare_tunnel import start_tunnel\n",
    "from manga_ai.scripts.gradio_app import launch_gradio\n",
    "\n",
    "PORT = int(os.environ.get(\"MANGA_AI_PORT\", \"7860\"))\n",
    "HOST = os.environ.get(\"MANGA_AI_HOST\", \"127.0.0.1\")\n",
    "\n",
    "launch_gradio(root=ROOT, host=HOST, port=PORT)\n",
    "\n",
    "tunnel = start_tunnel(local_port=PORT, cache_dir=ROOT / \"outputs\" / \"cloudflared\", host=HOST)\n",
    "print(\"Public URL:\", tunnel.public_url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
